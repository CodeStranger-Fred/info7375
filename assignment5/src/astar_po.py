# src/astar_po.py
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Dict, Any, Tuple
import numpy as np
import re

class AStarPO:
    def __init__(self, model, tokenizer, beta: float = 0.1, num_samples: int = 8):
        self.model = model
        self.tokenizer = tokenizer
        self.beta = beta  # KLæƒ©ç½šç³»æ•°
        self.num_samples = num_samples
        
    def compute_rewards(self, responses: List[List[str]], targets: List[float], numbers_list: List[List[float]]) -> List[List[float]]:
        """è®¡ç®—å¥–åŠ±åˆ†æ•°"""
        batch_rewards = []
        
        for i, response_list in enumerate(responses):
            target = targets[i]
            numbers = numbers_list[i]
            rewards = []
            
            for response in response_list:
                # æå–ç­”æ¡ˆå¹¶éªŒè¯
                reward = self._validate_response(response, target, numbers)
                rewards.append(reward)
            
            batch_rewards.append(rewards)
        
        return batch_rewards
    
    def _validate_response(self, response: str, target: float, numbers: List[float]) -> float:
        """éªŒè¯å“åº”å¹¶è®¡ç®—å¥–åŠ± - ç»†ç²’åº¦ç‰ˆæœ¬
        
        å¥–åŠ±ç»„æˆï¼š
        - ç©ºå“åº”/æ— æ•ˆå“åº”: -0.5 (ä¸¥é‡æƒ©ç½šï¼)
        - æ ¼å¼æ­£ç¡®ï¼ˆæœ‰<answer>æ ‡ç­¾ï¼‰: +0.1
        - å¯ä»¥è§£æçš„è¡¨è¾¾å¼: +0.1
        - åªä½¿ç”¨å…è®¸çš„è¿ç®—ç¬¦: +0.1
        - æ•°å­—ä½¿ç”¨æ¥è¿‘æ­£ç¡®: 0.0~0.2ï¼ˆæ ¹æ®å¤šç”¨/å°‘ç”¨ç¨‹åº¦ï¼‰
        - ç»“æœæ¥è¿‘ç›®æ ‡: 0.0~0.6ï¼ˆæ ¹æ®ç›¸å¯¹è¯¯å·®ï¼‰
        - å®Œå…¨æ­£ç¡®: 1.0
        """
        reward = 0.0
        equation = None
        
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯ç©ºå“åº”
        if not response or response.strip() == "":
            return -0.5  # ä¸¥é‡æƒ©ç½šç©ºå“åº”
        
        try:
            # 1. å°è¯•æå–<answer>æ ‡ç­¾
            answer_match = re.search(r'<answer>(.*?)</answer>', response, re.DOTALL)
            if answer_match:
                reward += 0.1  # æ ¼å¼å¥–åŠ±
                equation = answer_match.group(1).strip()
                # æ£€æŸ¥æ ‡ç­¾å†…æ˜¯å¦ä¸ºç©º
                if not equation:
                    return -0.3  # ç©ºæ ‡ç­¾ä¹Ÿè¦æƒ©ç½š
            else:
                # å¦‚æœæ²¡æœ‰<answer>æ ‡ç­¾ï¼Œå°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”
                # æå–ç¬¬ä¸€ä¸ªçœ‹èµ·æ¥åƒè®¡ç®—å¼çš„å­—ç¬¦ä¸²
                # åŒ¹é…æ•°å­—å’Œè¿ç®—ç¬¦çš„ç»„åˆ
                equation_pattern = r'([\d\s\+\-\*\/\(\)]+)'
                matches = re.findall(equation_pattern, response)
                if matches:
                    # å–æœ€é•¿çš„åŒ¹é…é¡¹
                    equation = max(matches, key=len).strip()
                    # éªŒè¯æ˜¯å¦æ˜¯æœ‰æ•ˆè®¡ç®—å¼ï¼ˆè‡³å°‘åŒ…å«ä¸€ä¸ªæ•°å­—å’Œä¸€ä¸ªè¿ç®—ç¬¦ï¼‰
                    if len(re.findall(r'\d', equation)) > 0 and any(op in equation for op in ['+', '-', '*', '/']):
                        reward += 0.05  # ç»™ä¸€äº›å¥–åŠ±ï¼Œä½†æ¯”æ­£ç¡®æ ¼å¼å°‘
                    else:
                        equation = None
            
            if not equation:
                return -0.3  # æ²¡æ‰¾åˆ°ä»»ä½•è®¡ç®—å¼ï¼Œè´Ÿå¥–åŠ±
            
            # 2. æ£€æŸ¥æ˜¯å¦åªä½¿ç”¨å…è®¸çš„è¿ç®—ç¬¦å’Œæ•°å­—
            # ç§»é™¤ç©ºæ ¼å’Œæ‹¬å·ï¼Œæ£€æŸ¥å‰©ä½™å­—ç¬¦
            clean_eq = equation.replace(' ', '').replace('(', '').replace(')', '')
            allowed_chars = set('0123456789+-*/.') 
            if all(c in allowed_chars for c in clean_eq):
                reward += 0.1  # è¿ç®—ç¬¦åˆæ³•
            
            # 3. æå–å¹¶éªŒè¯æ•°å­—ä½¿ç”¨
            used_numbers = self._extract_numbers(equation)
            
            # è®¡ç®—æ•°å­—ä½¿ç”¨çš„åŒ¹é…åº¦
            if len(used_numbers) == len(numbers):
                # æ•°å­—ä¸ªæ•°æ­£ç¡®
                used_sorted = sorted(used_numbers)
                allowed_sorted = sorted(numbers)
                # æ£€æŸ¥æœ‰å¤šå°‘æ•°å­—åŒ¹é…
                matches = sum(1 for u, a in zip(used_sorted, allowed_sorted) if abs(u - a) < 1e-6)
                reward += 0.2 * (matches / len(numbers))  # æŒ‰åŒ¹é…æ¯”ä¾‹ç»™åˆ†
            elif len(used_numbers) < len(numbers):
                # å°‘ç”¨äº†æ•°å­—
                reward += 0.1 * (len(used_numbers) / len(numbers))
            else:
                # å¤šç”¨äº†æ•°å­—ï¼Œè½»å¾®æƒ©ç½šä½†ä¸å½’é›¶
                reward += 0.05
            
            # 4. å°è¯•è®¡ç®—è¡¨è¾¾å¼
            result = self._evaluate_equation(equation)
            if result == float('inf') or result != result:  # inf or nan
                return reward  # è¿”å›åˆ°ç›®å‰ä¸ºæ­¢çš„å¥–åŠ±
            
            reward += 0.1  # æˆåŠŸè®¡ç®—
            
            # 5. æ ¹æ®ç»“æœå‡†ç¡®åº¦ç»™å¥–åŠ±ï¼ˆæ›´å®½æ¾çš„å¥–åŠ±ï¼‰
            error = abs(result - target)
            
            if error < 1e-6:
                # å®Œå…¨æ­£ç¡®ï¼
                return 1.0
            elif target != 0:
                # æ ¹æ®ç›¸å¯¹è¯¯å·®ç»™å¥–åŠ±ï¼ˆåˆç†çš„æ¢¯åº¦è¯„åˆ†ï¼‰
                relative_error = error / abs(target)
                
                # æ ¹æ®è¯¯å·®å¤§å°ç»™äºˆé€’å‡çš„å¥–åŠ±
                if relative_error < 0.05:  # è¯¯å·® < 5%
                    accuracy_reward = 0.6  # éå¸¸æ¥è¿‘ï¼
                elif relative_error < 0.1:  # è¯¯å·® < 10%
                    accuracy_reward = 0.5  # å¾ˆæ¥è¿‘
                elif relative_error < 0.2:  # è¯¯å·® < 20%
                    accuracy_reward = 0.4  # æ¯”è¾ƒæ¥è¿‘
                elif relative_error < 0.5:  # è¯¯å·® < 50%
                    accuracy_reward = 0.2  # æœ‰ç‚¹æ¥è¿‘
                elif relative_error < 1.0:  # è¯¯å·® < 100%
                    accuracy_reward = 0.1  # è‡³å°‘åœ¨åŒä¸€æ•°é‡çº§
                else:
                    accuracy_reward = 0.05  # å¾ˆè¿œä½†è‡³å°‘æœ‰å°è¯•
                
                reward += accuracy_reward
            else:
                # targetæ˜¯0çš„ç‰¹æ®Šæƒ…å†µ
                if error < 1:
                    reward += 0.6 * (1 - error)
                else:
                    reward += 0.1  # åŸºæœ¬åˆ†
            
            return min(reward, 0.95)  # ä¸å®Œå…¨æ­£ç¡®çš„æœ€é«˜åˆ†æ˜¯0.95
            
        except Exception as e:
            # å³ä½¿å‡ºé”™ï¼Œä¹Ÿè¿”å›åˆ°ç›®å‰ä¸ºæ­¢ç´¯ç§¯çš„å¥–åŠ±
            return reward
    
    def _extract_numbers(self, equation: str) -> List[float]:
        """ä»æ–¹ç¨‹ä¸­æå–ä½¿ç”¨çš„æ•°å­—"""
        numbers = re.findall(r'\d+\.?\d*', equation)
        return [float(num) for num in numbers]
    
    def _validate_number_usage(self, used_numbers: List[float], allowed_numbers: List[float]) -> bool:
        """éªŒè¯æ•°å­—ä½¿ç”¨æ˜¯å¦ç¬¦åˆè§„åˆ™"""
        if len(used_numbers) != len(allowed_numbers):
            return False
        
        used_sorted = sorted(used_numbers)
        allowed_sorted = sorted(allowed_numbers)
        
        return all(abs(u - a) < 1e-6 for u, a in zip(used_sorted, allowed_sorted))
    
    def _evaluate_equation(self, equation: str) -> float:
        """å®‰å…¨åœ°è¯„ä¼°æ•°å­¦è¡¨è¾¾å¼"""
        # ç§»é™¤å±é™©æ“ä½œ
        equation = equation.replace('import', '').replace('exec', '').replace('eval', '')
        
        try:
            # ä½¿ç”¨evalè®¡ç®—ï¼Œä½†åœ¨ç”Ÿäº§ç¯å¢ƒä¸­åº”è¯¥ä½¿ç”¨æ›´å®‰å…¨çš„è¯„ä¼°æ–¹æ³•
            result = eval(equation, {"__builtins__": None}, {})
            return float(result)
        except:
            return float('inf')  # è¿”å›æ— ç©·å¤§è¡¨ç¤ºè®¡ç®—é”™è¯¯
    
    def compute_loss(self, prompts: List[str], responses: List[List[str]], 
                    rewards: List[List[float]], reference_logprobs: List[List[torch.Tensor]]) -> torch.Tensor:
        """è®¡ç®—A*POæŸå¤±ï¼ŒåŒ…å«KLæ•£åº¦æƒ©ç½šå’Œadvantageæ ‡å‡†åŒ–
        
        Loss = -E[A * log Ï€(y|x)] + Î² * KL(Ï€ || Ï€_ref)
        å…¶ä¸­ï¼š
        - A = (r - r_mean) / (r_std + Îµ) æ˜¯æ ‡å‡†åŒ–åçš„advantage
        - Ï€ æ˜¯å½“å‰ç­–ç•¥
        - Ï€_ref æ˜¯å‚è€ƒç­–ç•¥
        - Î² æ˜¯KLæƒ©ç½šç³»æ•°
        """
        # 1. æ”¶é›†æ‰€æœ‰rewardç”¨äºæ ‡å‡†åŒ–
        all_rewards = []
        for reward_list in rewards:
            all_rewards.extend(reward_list)
        
        # 2. ä½¿ç”¨ç›¸å¯¹ä¼˜åŠ¿ï¼ˆä¸å®Œå…¨æ ‡å‡†åŒ–ï¼Œä¿æŒrewardçš„æ­£è´Ÿæ€§ï¼‰
        if len(all_rewards) > 1:
            reward_mean = np.mean(all_rewards)
            reward_std = np.std(all_rewards)
            
            # ä½¿ç”¨advantage = r - baselineï¼Œä½†ä¸é™¤ä»¥std
            # è¿™æ ·å¯ä»¥ä¿æŒrewardçš„å¤§å°å…³ç³»ï¼ŒåŒæ—¶å‡å°‘æ–¹å·®
            normalized_rewards = [
                [r - reward_mean for r in reward_list]
                for reward_list in rewards
            ]
            
            # åœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶æ˜¾ç¤ºä¿¡æ¯
            if not hasattr(self, '_shown_norm_info'):
                print(f"\nğŸ“ˆ Reward Stats: mean={reward_mean:.4f}, std={reward_std:.4f}")
                print(f"   Using advantage = reward - mean (not dividing by std)")
                self._shown_norm_info = True
        else:
            # å¦‚æœåªæœ‰ä¸€ä¸ªrewardï¼Œä¸åšæ ‡å‡†åŒ–
            normalized_rewards = rewards
        
        policy_loss = torch.tensor(0.0, device=self.model.device, requires_grad=True)
        kl_loss = torch.tensor(0.0, device=self.model.device, requires_grad=True)
        count = 0
        
        for i, prompt in enumerate(prompts):
            prompt_responses = responses[i]
            prompt_rewards = normalized_rewards[i]  # ä½¿ç”¨æ ‡å‡†åŒ–åçš„reward
            ref_logprobs = reference_logprobs[i]
            
            # Tokenize prompt
            prompt_tokens = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=200).to(self.model.device)
            
            # ä¸ºæ¯ä¸ªå“åº”è®¡ç®—æŸå¤±
            for j, (response, reward, ref_logprob) in enumerate(zip(prompt_responses, prompt_rewards, ref_logprobs)):
                # Tokenize full sequence (prompt + response)
                full_text = prompt + response
                tokens = self.tokenizer(full_text, return_tensors="pt", truncation=True, max_length=400).to(self.model.device)
                
                # è®¡ç®—logits (ä¿ç•™æ¢¯åº¦)
                outputs = self.model(**tokens)
                logits = outputs.logits
                
                # è®¡ç®—log probs
                log_probs = F.log_softmax(logits, dim=-1)
                
                # åªåœ¨responseéƒ¨åˆ†è®¡ç®—loss
                prompt_length = prompt_tokens.input_ids.shape[1]
                if tokens.input_ids.shape[1] > prompt_length + 1:
                    # è®¡ç®—response tokensçš„log probs
                    response_logprobs = []
                    for t in range(prompt_length, tokens.input_ids.shape[1] - 1):
                        token_id = tokens.input_ids[0, t + 1]
                        token_logprob = log_probs[0, t, token_id]
                        response_logprobs.append(token_logprob)
                    
                    if len(response_logprobs) > 0:
                        # å½“å‰ç­–ç•¥çš„log prob
                        seq_logprob = torch.stack(response_logprobs).mean()
                        
                        # ç­–ç•¥æ¢¯åº¦æŸå¤±ï¼š-reward * log_prob
                        reward_tensor = torch.tensor(reward, device=self.model.device)
                        policy_loss = policy_loss + (-reward_tensor * seq_logprob)
                        
                        # KLæ•£åº¦æƒ©ç½š: KL(Ï€ || Ï€_ref) = log(Ï€) - log(Ï€_ref)
                        ref_logprob_tensor = ref_logprob.to(self.model.device)
                        kl_divergence = seq_logprob - ref_logprob_tensor
                        kl_loss = kl_loss + kl_divergence
                        
                        count += 1
        
        # æ€»æŸå¤± = ç­–ç•¥æŸå¤± + Î² * KLæŸå¤±
        if count > 0:
            avg_policy_loss = policy_loss / count
            avg_kl_loss = kl_loss / count
            
            # KLæƒ©ç½šé¡¹ï¼ˆé˜²æ­¢ç­–ç•¥åç¦»å¤ªè¿œï¼‰
            kl_penalty = self.beta * avg_kl_loss
            total_loss = avg_policy_loss + kl_penalty
            
            # è®°å½•ç»Ÿè®¡ä¿¡æ¯ï¼ˆç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶ï¼‰
            if not hasattr(self, '_shown_kl_info'):
                print(f"\nğŸ’œ KL Divergence Info:")
                print(f"   Policy Loss: {avg_policy_loss.item():.4f}")
                print(f"   KL Divergence: {avg_kl_loss.item():.4f}")
                print(f"   KL Penalty (Î²={self.beta}): {kl_penalty.item():.4f}")
                print(f"   Total Loss: {total_loss.item():.4f}")
                self._shown_kl_info = True
        else:
            total_loss = policy_loss
        
        return total_loss
