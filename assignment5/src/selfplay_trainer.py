# src/selfplay_trainer.py
import torch
import torch.nn.functional as F
import numpy as np
from tqdm import tqdm
from typing import List
from src.astar_po import AStarPO
from src.llm_problem_generator import LLMProblemGenerator

class SelfPlayTrainer:
    """
    Self-Playè®­ç»ƒå™¨
    æ¨¡å‹æ—¢æ˜¯å‡ºé¢˜è€…åˆæ˜¯ç­”é¢˜è€…
    """
    
    def __init__(self, model, tokenizer, config):
        self.model = model
        self.tokenizer = tokenizer
        self.config = config
        
        self.optimizer = torch.optim.AdamW(
            model.parameters(), 
            lr=config.get('learning_rate', 5e-5)
        )
        
        self.astar_po = AStarPO(
            model, 
            tokenizer,
            beta=config.get('beta', 0.1),
            num_samples=config.get('num_samples', 8)
        )
        
        # åˆ›å»ºå‚è€ƒæ¨¡å‹ç”¨äºEMAæ›´æ–°
        print("ğŸ“‹ Creating reference model copy...")
        import copy
        self.ref_model = copy.deepcopy(model)
        self.ref_model.eval()
        for param in self.ref_model.parameters():
            param.requires_grad = False
        print("âœ… Reference model created (frozen)")
        
        # EMAæ›´æ–°å‚æ•°
        self.ema_decay = config.get('ema_decay', 0.95)
        
        # å‚è€ƒæ¨¡å‹æ›´æ–°é¢‘ç‡
        self.ref_update_frequency = config.get('ref_update_frequency', 1000)
        self.last_ref_update = 0
        
        # LLMé—®é¢˜ç”Ÿæˆå™¨ï¼ˆSelf-playæ ¸å¿ƒï¼‰
        use_llm_gen = config.get('use_llm_generator', False)
        self.use_llm_generator = use_llm_gen
        
        if use_llm_gen:
            print("ğŸ® Initializing LLM Self-Play Generator...")
            self.problem_generator = LLMProblemGenerator(
                model, 
                tokenizer, 
                difficulty_curriculum=config.get('difficulty_curriculum', True)
            )
            print("âœ… Self-Play mode activated!")
        else:
            # å›é€€åˆ°rule-based
            from src.online_problem_generator import OnlineProblemGenerator
            self.problem_generator = OnlineProblemGenerator()
            print("ğŸ“Š Using rule-based generator")
        
        self.global_step = 0
        self.best_reward = 0.0
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.llm_generated_count = 0
        self.rule_based_count = 0
    
    def train_iteration(self, iteration: int, num_problems: int) -> tuple:
        """
        è®­ç»ƒä¸€ä¸ªè¿­ä»£
        
        Self-Playæµç¨‹ï¼š
        1. è®©æ¨¡å‹ç”Ÿæˆé—®é¢˜ï¼ˆå‡ºé¢˜è€…ï¼‰
        2. è®©æ¨¡å‹è§£å†³é—®é¢˜ï¼ˆç­”é¢˜è€…ï¼‰
        3. æ ¹æ®ç­”é¢˜è´¨é‡æ›´æ–°æ¨¡å‹
        """
        self.model.train()
        
        iteration_loss = 0.0
        iteration_reward = 0.0
        
        pbar = tqdm(range(num_problems), desc=f"Iteration {iteration}")
        
        saved_outputs = []
        
        for problem_idx in pbar:
            # 1. ç”Ÿæˆé—®é¢˜ï¼ˆSelf-play: æ¨¡å‹ä½œä¸ºå‡ºé¢˜è€…ï¼‰
            if self.use_llm_generator:
                problem = self.problem_generator.generate_problem_with_llm()
                if problem.get('source') == 'llm_generated':
                    self.llm_generated_count += 1
                else:
                    self.rule_based_count += 1
            else:
                problem = self.problem_generator.generate_problem()
                self.rule_based_count += 1
            
            prompt = self.problem_generator.make_prompt(problem)
            target = problem['target']
            numbers = problem['nums']
            
            # æ‰“å°ç”Ÿæˆçš„é—®é¢˜
            print(f"\n{'='*60}")
            source_label = problem.get('source', 'rule-based')
            print(f"ğŸ“ é—®é¢˜ {problem_idx+1} ({source_label}): æ•°å­—={numbers}, ç›®æ ‡={target}")
            print(f"æç¤ºè¯: {prompt[:100]}..." if len(prompt) > 100 else f"æç¤ºè¯: {prompt}")
            
            # 2. è§£å†³é—®é¢˜ï¼ˆSelf-play: æ¨¡å‹ä½œä¸ºç­”é¢˜è€…ï¼‰
            responses = []
            inputs = self.tokenizer(
                prompt, 
                return_tensors="pt", 
                truncation=True, 
                max_length=256
            ).to(self.model.device)
            
            # æ¸©åº¦é€€ç«ï¼šéšç€è®­ç»ƒè¿›å±•é€æ¸é™ä½é‡‡æ ·æ¸©åº¦
            temperature = self._get_temperature()
            
            for _ in range(self.astar_po.num_samples):
                with torch.no_grad():
                    outputs = self.model.generate(
                        inputs.input_ids,
                        attention_mask=inputs.attention_mask,
                        max_new_tokens=self.config.get('max_length', 256),
                        num_return_sequences=1,
                        temperature=temperature,  # ä½¿ç”¨åŠ¨æ€æ¸©åº¦
                        do_sample=True,
                        pad_token_id=self.tokenizer.eos_token_id,
                        eos_token_id=self.tokenizer.eos_token_id,
                    )
                
                response = self.tokenizer.decode(
                    outputs[0][inputs.input_ids.shape[1]:], 
                    skip_special_tokens=True
                )
                responses.append(response)
            
            # æ‰“å°æ¨¡å‹ç”Ÿæˆçš„æ‰€æœ‰ç­”æ¡ˆ
            print(f"\nğŸ¤– æ¨¡å‹ç”Ÿæˆçš„ {len(responses)} ä¸ªç­”æ¡ˆ:")
            for i, resp in enumerate(responses, 1):
                print(f"  ç­”æ¡ˆ{i}: {resp[:80]}..." if len(resp) > 80 else f"  ç­”æ¡ˆ{i}: {resp}")
            
            # 3. è®¡ç®—å¥–åŠ±
            rewards = self.astar_po.compute_rewards(
                [responses], [target], [numbers]
            )[0]
            
            # 4. è®¡ç®—å‚è€ƒç­–ç•¥çš„logprobs
            reference_logprobs = self._compute_reference_logprobs([responses])[0]
            
            # å®šæœŸæ›´æ–°å‚è€ƒæ¨¡å‹
            if self.global_step - self.last_ref_update >= self.ref_update_frequency:
                self._update_reference_model()
                self.last_ref_update = self.global_step
            
            # 5. è®¡ç®—æŸå¤±å¹¶æ›´æ–°
            loss = self.astar_po.compute_loss(
                [prompt], [responses], [rewards], [reference_logprobs]
            )
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self.optimizer.step()
            
            # è®°å½•ç»Ÿè®¡ä¿¡æ¯
            batch_reward = np.mean(rewards)
            iteration_loss += loss.item()
            iteration_reward += batch_reward
            
            # ä¿å­˜å‰3ä¸ªé—®é¢˜çš„è¯¦ç»†è¾“å‡º
            if problem_idx < 3:
                saved_outputs.append({
                    "iteration": iteration,
                    "problem_idx": problem_idx,
                    "problem": problem,
                    "target": target,
                    "numbers": numbers,
                    "responses": responses,
                    "rewards": rewards,
                    "avg_reward": float(batch_reward),
                    "source": problem.get('source', 'unknown')
                })
            
            # æ›´æ–°è¿›åº¦æ¡
            postfix = {
                'loss': f'{loss.item():.4f}',
                'reward': f'{batch_reward:.4f}'
            }
            if self.use_llm_generator:
                postfix['llm_gen'] = f'{self.llm_generated_count}/{self.llm_generated_count + self.rule_based_count}'
            
            pbar.set_postfix(postfix)
            
            self.global_step += 1
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if self.global_step % self.config.get('save_steps', 100) == 0:
                self._save_checkpoint(iteration, iteration_loss / (problem_idx + 1))
        
        # ä¿å­˜è¯¦ç»†è¾“å‡ºåˆ°æ–‡ä»¶
        if saved_outputs:
            import json
            output_file = f'outputs_selfplay_iter_{iteration}.json'
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(saved_outputs, f, indent=2, ensure_ascii=False)
            print(f"\nğŸ’¾ Saved outputs to {output_file}")
        
        # æ‰“å°Self-playç»Ÿè®¡
        if self.use_llm_generator:
            total = self.llm_generated_count + self.rule_based_count
            llm_ratio = self.llm_generated_count / total if total > 0 else 0
            print(f"\nğŸ® Self-Play Stats: {self.llm_generated_count} LLM-generated ({llm_ratio*100:.1f}%), {self.rule_based_count} rule-based")
            
            if hasattr(self.problem_generator, 'get_stats'):
                gen_stats = self.problem_generator.get_stats()
                print(f"ğŸ“ˆ Current difficulty: {gen_stats.get('current_difficulty', 'N/A')}")
        
        avg_loss = iteration_loss / num_problems
        avg_reward = iteration_reward / num_problems
        
        return avg_loss, avg_reward
    
    def _compute_reference_logprobs(self, responses: List[List[str]]) -> List[List[torch.Tensor]]:
        """è®¡ç®—å‚è€ƒç­–ç•¥çš„logæ¦‚ç‡ï¼ˆä½¿ç”¨å›ºå®šçš„å‚è€ƒæ¨¡å‹ï¼‰"""
        reference_logprobs = []
        
        for prompt_responses in responses:
            prompt_logprobs = []
            for response in prompt_responses:
                tokens = self.tokenizer.encode(response, return_tensors="pt")
                if tokens.shape[1] == 0:
                    prompt_logprobs.append(torch.tensor(0.0))
                    continue
                    
                tokens = tokens.to(self.ref_model.device)
                with torch.no_grad():
                    outputs = self.ref_model(tokens)
                    logits = outputs.logits
                    logprobs = F.log_softmax(logits, dim=-1)
                    
                    if tokens.shape[1] > 1:
                        token_logprobs = torch.gather(
                            logprobs[:-1], 2, tokens[1:].unsqueeze(-1)
                        ).squeeze(-1)
                        seq_logprob = token_logprobs.sum()
                    else:
                        seq_logprob = torch.tensor(0.0)
                    
                    prompt_logprobs.append(seq_logprob.cpu())
            reference_logprobs.append(prompt_logprobs)
        
        return reference_logprobs
    
    def _get_temperature(self) -> float:
        """
        è®¡ç®—å½“å‰çš„é‡‡æ ·æ¸©åº¦ï¼ˆæ¸©åº¦é€€ç«ï¼‰
        
        éšç€è®­ç»ƒè¿›å±•ï¼Œé€æ¸ä»é«˜æ¸©åº¦ï¼ˆå¤šæ ·æ€§ï¼‰è¿‡æ¸¡åˆ°ä½æ¸©åº¦ï¼ˆç¨³å®šæ€§ï¼‰
        
        é«˜æ¸©åº¦ (1.0): æ¢ç´¢ï¼Œå¤šæ ·æ€§é«˜
        ä½æ¸©åº¦ (0.3): åˆ©ç”¨ï¼Œè¾“å‡ºç¨³å®š
        """
        initial_temp = self.config.get('initial_temperature', 1.0)
        min_temp = self.config.get('min_temperature', 0.3)
        decay_rate = self.config.get('temperature_decay_rate', 1e-5)
        
        # çº¿æ€§é€€ç«ï¼štemp = initial - decay_rate * step
        temperature = max(min_temp, initial_temp - decay_rate * self.global_step)
        
        # åœ¨ç¬¬ä¸€æ¬¡å’Œæ¯1000æ­¥æ˜¾ç¤ºæ¸©åº¦
        if not hasattr(self, '_shown_temp') or self.global_step % 1000 == 0:
            if not hasattr(self, '_shown_temp'):
                print(f"\nğŸŒ¡ï¸  Temperature Annealing: {initial_temp} â†’ {min_temp} (decay={decay_rate})")
                self._shown_temp = True
            if self.global_step % 1000 == 0 and self.global_step > 0:
                print(f"\nğŸŒ¡ï¸  Temperature at step {self.global_step}: {temperature:.3f}")
        
        return temperature
    
    def _update_reference_model(self):
        """ä½¿ç”¨EMAé€å‚æ•°æ›´æ–°å‚è€ƒæ¨¡å‹ï¼ˆé¿å…æ˜¾å­˜å³°å€¼ï¼‰"""
        print(f"\nğŸ”„ Updating reference model with EMA (decay={self.ema_decay}) at step {self.global_step}...")
        
        # é€å‚æ•°EMAæ›´æ–°ï¼šp_ref = decay * p_ref + (1-decay) * p
        with torch.no_grad():
            for p_ref, p in zip(self.ref_model.parameters(), self.model.parameters()):
                p_ref.data.mul_(self.ema_decay).add_(p.data, alpha=1.0 - self.ema_decay)
        
        print("âœ… Reference model updated with EMA (no memory spike!)")
    
    def _save_checkpoint(self, iteration: int, loss: float):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        import os
        from datetime import datetime
        
        checkpoint = {
            'iteration': iteration,
            'global_step': self.global_step,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'loss': loss,
            'llm_generated_count': self.llm_generated_count,
            'rule_based_count': self.rule_based_count,
            'timestamp': datetime.now().isoformat()
        }
        
        os.makedirs('checkpoints', exist_ok=True)
        checkpoint_path = f"checkpoints/selfplay_checkpoint_iter_{iteration}_step_{self.global_step}.pt"
        torch.save(checkpoint, checkpoint_path)
        print(f"ğŸ’¾ Checkpoint saved: {checkpoint_path}")
