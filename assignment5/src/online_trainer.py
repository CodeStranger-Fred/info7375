# src/online_trainer.py
import torch
import torch.nn.functional as F
import numpy as np
from tqdm import tqdm
from typing import List
from src.astar_po import AStarPO
from src.online_problem_generator import OnlineProblemGenerator
from src.detailed_logger import DetailedLogger

class OnlineRolloutTrainer:
    """TinyZeroé£æ ¼çš„Online Rolloutè®­ç»ƒå™¨"""
    
    def __init__(self, model, tokenizer, config):
        self.model = model
        self.tokenizer = tokenizer
        self.config = config
        
        # åˆå§‹åŒ–è¯¦ç»†æ—¥å¿—å™¨
        self.detailed_logger = DetailedLogger(log_dir=config.get('log_dir', 'detailed_logs'))
        print("âœ… Detailed logger initialized")
        
        # Onlineé—®é¢˜ç”Ÿæˆå™¨ - å¿…é¡»åœ¨SFT warmupä¹‹å‰åˆå§‹åŒ–
        self.problem_generator = OnlineProblemGenerator()
        
        self.optimizer = torch.optim.AdamW(
            model.parameters(), 
            lr=config.get('learning_rate', 5e-5)
        )
        
        self.astar_po = AStarPO(
            model, 
            tokenizer,
            beta=config.get('beta', 0.1),
            num_samples=config.get('num_samples', 8)
        )
        
        # åˆ›å»ºå‚è€ƒæ¨¡å‹ç”¨äºEMAæ›´æ–°
        print("ğŸ“‹ Creating reference model copy...")
        import copy
        self.ref_model = copy.deepcopy(model)
        self.ref_model.eval()
        for param in self.ref_model.parameters():
            param.requires_grad = False
        print("âœ… Reference model created (frozen)")
        
        # EMAæ›´æ–°å‚æ•°
        self.ema_decay = config.get('ema_decay', 0.95)
        
        # å‚è€ƒæ¨¡å‹æ›´æ–°é¢‘ç‡
        self.ref_update_frequency = config.get('ref_update_frequency', 1000)
        self.last_ref_update = 0
        
        # SFT Warmup: ç”¨æ­£ç¡®ç­”æ¡ˆå…ˆè®­ç»ƒå‡ æ­¥ï¼ˆåœ¨åˆå§‹åŒ–å®Œæˆåï¼‰
        warmup_steps = config.get('sft_warmup_steps', 0)
        if warmup_steps > 0:
            warmup_loss = self._sft_warmup(warmup_steps)
            self.detailed_logger.log_warmup({
                "num_steps": warmup_steps,
                "avg_loss": warmup_loss
            })
        
        self.global_step = 0
        self.best_reward = 0.0
    
    def train_iteration(self, iteration: int, num_problems: int) -> tuple:
        """
        è®­ç»ƒä¸€ä¸ªè¿­ä»£
        
        Online Rolloutæµç¨‹ï¼š
        1. åŠ¨æ€ç”Ÿæˆæ–°é—®é¢˜
        2. ç”¨å½“å‰ç­–ç•¥rolloutï¼ˆé‡‡æ ·å¤šä¸ªå“åº”ï¼‰
        3. è®¡ç®—å¥–åŠ±
        4. æ›´æ–°ç­–ç•¥
        """
        self.model.train()
        
        # è®°å½•è¿­ä»£å¼€å§‹
        self.detailed_logger.log_iteration_start(iteration, num_problems)
        
        iteration_loss = 0.0
        iteration_reward = 0.0
        
        pbar = tqdm(range(num_problems), desc=f"Iteration {iteration}")
        
        # ç”¨äºä¿å­˜è¾“å‡ºæ ·ä¾‹
        saved_outputs = []
        
        for problem_idx in pbar:
            # 1. åŠ¨æ€ç”Ÿæˆæ–°é—®é¢˜
            problem = self.problem_generator.generate_problem()
            prompt = self.problem_generator.make_prompt(problem)
            target = problem['target']
            numbers = problem['nums']
            
            # æ‰“å°ç”Ÿæˆçš„é—®é¢˜
            print(f"\n{'='*60}")
            print(f"ğŸ“ é—®é¢˜ {problem_idx+1}: æ•°å­—={numbers}, ç›®æ ‡={target}")
            print(f"æç¤ºè¯: {prompt[:100]}..." if len(prompt) > 100 else f"æç¤ºè¯: {prompt}")
            
            # 2. ç”¨å½“å‰ç­–ç•¥rolloutï¼šç”Ÿæˆå¤šä¸ªå“åº”
            responses = []
            inputs = self.tokenizer(
                prompt, 
                return_tensors="pt", 
                truncation=True, 
                max_length=256
            ).to(self.model.device)
            
            # æ¸©åº¦é€€ç«ï¼šéšç€è®­ç»ƒè¿›å±•é€æ¸é™ä½é‡‡æ ·æ¸©åº¦
            temperature = self._get_temperature()
            
            # ä¸º </answer> åˆ›å»º stopping criteria
            answer_end_token = "</answer>"
            answer_end_id = self.tokenizer.encode(answer_end_token, add_special_tokens=False)
            
            for _ in range(self.astar_po.num_samples):
                with torch.no_grad():
                    outputs = self.model.generate(
                        inputs.input_ids,
                        attention_mask=inputs.attention_mask,
                        max_new_tokens=self.config.get('max_length', 50),
                        num_return_sequences=1,
                        temperature=temperature,  # ä½¿ç”¨åŠ¨æ€æ¸©åº¦
                        do_sample=True,
                        repetition_penalty=1.2,  # é˜²æ­¢é‡å¤
                        pad_token_id=self.tokenizer.eos_token_id,
                        eos_token_id=self.tokenizer.eos_token_id,
                    )
                
                response = self.tokenizer.decode(
                    outputs[0][inputs.input_ids.shape[1]:], 
                    skip_special_tokens=True
                )
                responses.append(response)
            
            # æ‰“å°æ¨¡å‹ç”Ÿæˆçš„æ‰€æœ‰ç­”æ¡ˆ
            print(f"\nğŸ¤– æ¨¡å‹ç”Ÿæˆçš„ {len(responses)} ä¸ªç­”æ¡ˆ:")
            for i, resp in enumerate(responses, 1):
                print(f"  ç­”æ¡ˆ{i}: {resp[:80]}..." if len(resp) > 80 else f"  ç­”æ¡ˆ{i}: {resp}")
            
            # 3. è®¡ç®—å¥–åŠ±
            rewards = self.astar_po.compute_rewards(
                [responses], [target], [numbers]
            )[0]  # å–ç¬¬ä¸€ä¸ªï¼ˆå› ä¸ºbatch_size=1ï¼‰
            
            # 4. è®¡ç®—å‚è€ƒç­–ç•¥çš„logprobsï¼ˆä½¿ç”¨å›ºå®šçš„å‚è€ƒæ¨¡å‹ï¼‰
            reference_logprobs = self._compute_reference_logprobs([responses])[0]
            
            # å®šæœŸæ›´æ–°å‚è€ƒæ¨¡å‹
            if self.global_step - self.last_ref_update >= self.ref_update_frequency:
                self._update_reference_model()
                self.detailed_logger.log_reference_model_update(self.global_step)
                self.last_ref_update = self.global_step
            
            # 5. è®¡ç®—æŸå¤±å¹¶æ›´æ–°
            loss = self.astar_po.compute_loss(
                [prompt], [responses], [rewards], [reference_logprobs]
            )
            
            # è®°å½•è¯¦ç»†ä¿¡æ¯ï¼ˆè·å–æ›´å¤šç»†èŠ‚ï¼‰
            policy_loss, kl_div = self._get_loss_components([prompt], [responses], [rewards], [reference_logprobs])
            self.detailed_logger.log_problem_detail(
                iteration=iteration,
                problem_idx=problem_idx,
                problem=problem,
                responses=responses,
                rewards=rewards,
                reference_logprobs=[float(lp) for lp in reference_logprobs],
                loss=loss.item(),
                policy_loss=policy_loss,
                kl_divergence=kl_div
            )
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self.optimizer.step()
            
            # è®°å½•ç»Ÿè®¡ä¿¡æ¯
            batch_reward = np.mean(rewards)
            iteration_loss += loss.item()
            iteration_reward += batch_reward
            
            # ä¿å­˜å‰3ä¸ªé—®é¢˜çš„è¯¦ç»†è¾“å‡º
            if problem_idx < 3:
                saved_outputs.append({
                    "iteration": iteration,
                    "problem_idx": problem_idx,
                    "problem": problem,
                    "target": target,
                    "numbers": numbers,
                    "responses": responses,
                    "rewards": rewards,
                    "avg_reward": float(batch_reward)
                })
            
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'reward': f'{batch_reward:.4f}'
            })
            
            self.global_step += 1
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if self.global_step % self.config.get('save_steps', 100) == 0:
                self._save_checkpoint(iteration, iteration_loss / (problem_idx + 1))
        
        # ä¿å­˜è¯¦ç»†è¾“å‡ºåˆ°æ–‡ä»¶
        if saved_outputs:
            import json
            output_file = f'outputs_iteration_{iteration}.json'
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(saved_outputs, f, indent=2, ensure_ascii=False)
            print(f"\nğŸ’¾ Saved outputs to {output_file}")
        
        avg_loss = iteration_loss / num_problems
        avg_reward = iteration_reward / num_problems
        
        # è®°å½•è¿­ä»£ç»“æŸ
        self.detailed_logger.log_iteration_end(iteration, {
            "avg_loss": avg_loss,
            "avg_reward": avg_reward,
            "total_problems": num_problems
        })
        
        return avg_loss, avg_reward
    
    def _compute_reference_logprobs(self, responses: List[List[str]]) -> List[List[torch.Tensor]]:
        """è®¡ç®—å‚è€ƒç­–ç•¥çš„logæ¦‚ç‡ï¼ˆä½¿ç”¨å›ºå®šçš„å‚è€ƒæ¨¡å‹ï¼‰"""
        reference_logprobs = []
        
        for prompt_responses in responses:
            prompt_logprobs = []
            for response in prompt_responses:
                tokens = self.tokenizer.encode(response, return_tensors="pt")
                if tokens.shape[1] == 0:
                    prompt_logprobs.append(torch.tensor(0.0))
                    continue
                    
                tokens = tokens.to(self.ref_model.device)
                with torch.no_grad():
                    # ä½¿ç”¨å›ºå®šçš„å‚è€ƒæ¨¡å‹
                    outputs = self.ref_model(tokens)
                    logits = outputs.logits
                    logprobs = F.log_softmax(logits, dim=-1)
                    
                    if tokens.shape[1] > 1:
                        token_logprobs = torch.gather(
                            logprobs[:-1], 2, tokens[1:].unsqueeze(-1)
                        ).squeeze(-1)
                        seq_logprob = token_logprobs.sum()
                    else:
                        seq_logprob = torch.tensor(0.0)
                    
                    prompt_logprobs.append(seq_logprob.cpu())
            reference_logprobs.append(prompt_logprobs)
        
        return reference_logprobs
    
    def _get_temperature(self) -> float:
        """
        è®¡ç®—å½“å‰çš„é‡‡æ ·æ¸©åº¦ï¼ˆæ¸©åº¦é€€ç«ï¼‰
        
        éšç€è®­ç»ƒè¿›å±•ï¼Œé€æ¸ä»é«˜æ¸©åº¦ï¼ˆå¤šæ ·æ€§ï¼‰è¿‡æ¸¡åˆ°ä½æ¸©åº¦ï¼ˆç¨³å®šæ€§ï¼‰
        """
        initial_temp = self.config.get('initial_temperature', 1.0)
        min_temp = self.config.get('min_temperature', 0.3)
        decay_rate = self.config.get('temperature_decay_rate', 1e-5)
        
        # çº¿æ€§é€€ç«
        temperature = max(min_temp, initial_temp - decay_rate * self.global_step)
        
        # åœ¨ç¬¬ä¸€æ¬¡å’Œæ¯1000æ­¥æ˜¾ç¤ºæ¸©åº¦
        if not hasattr(self, '_shown_temp') or self.global_step % 1000 == 0:
            if not hasattr(self, '_shown_temp'):
                print(f"\nğŸŒ¡ï¸  Temperature Annealing: {initial_temp} â†’ {min_temp} (decay={decay_rate})")
                self._shown_temp = True
            if self.global_step % 1000 == 0 and self.global_step > 0:
                print(f"\nğŸŒ¡ï¸  Temperature at step {self.global_step}: {temperature:.3f}")
        
        return temperature
    
    def _update_reference_model(self):
        """ä½¿ç”¨EMAé€å‚æ•°æ›´æ–°å‚è€ƒæ¨¡å‹ï¼ˆé¿å…æ˜¾å­˜å³°å€¼ï¼‰"""
        print(f"\nğŸ”„ Updating reference model with EMA (decay={self.ema_decay}) at step {self.global_step}...")
        
        # é€å‚æ•°EMAæ›´æ–°ï¼šp_ref = decay * p_ref + (1-decay) * p
        with torch.no_grad():
            for p_ref, p in zip(self.ref_model.parameters(), self.model.parameters()):
                p_ref.data.mul_(self.ema_decay).add_(p.data, alpha=1.0 - self.ema_decay)
        
        print("âœ… Reference model updated with EMA (no memory spike!)")
    
    def _sft_warmup(self, num_steps: int):
        """ç›‘ç£å­¦ä¹ çƒ­èº«ï¼šç”¨æ­£ç¡®ç­”æ¡ˆæ•™ä¼šæ¨¡å‹æ ¼å¼"""
        print(f"\nğŸ“ SFT Warmup: {num_steps} steps with correct answers...")
        
        self.model.train()
        warmup_optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-4)
        
        total_loss = 0.0
        
        for step in tqdm(range(num_steps), desc="SFT Warmup"):
            # ç”Ÿæˆé—®é¢˜
            problem = self.problem_generator.generate_problem()
            prompt = self.problem_generator.make_prompt(problem)
            
            # æ­£ç¡®ç­”æ¡ˆï¼šæ³¨æ„ prompt å·²ç»ä»¥ '<answer>' ç»“å°¾ï¼Œæ‰€ä»¥åªéœ€è¦è¡¥å…¨æ–¹ç¨‹å’Œç»“æŸæ ‡ç­¾
            correct_answer = f"{problem['solution']}</answer>"
            full_text = prompt + correct_answer
            
            # Tokenize prompt å’Œ full text
            prompt_tokens = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
            full_tokens = self.tokenizer(full_text, return_tensors="pt", truncation=True, max_length=512)
            
            # åˆ›å»º labelsï¼šåªåœ¨ answer éƒ¨åˆ†è®¡ç®— loss
            labels = full_tokens["input_ids"].clone()
            prompt_length = prompt_tokens["input_ids"].shape[1]
            labels[0, :prompt_length] = -100  # Mask prompt éƒ¨åˆ†
            
            inputs = {
                "input_ids": full_tokens["input_ids"].to(self.model.device),
                "attention_mask": full_tokens["attention_mask"].to(self.model.device),
                "labels": labels.to(self.model.device)
            }
            
            # ç›‘ç£å­¦ä¹ 
            outputs = self.model(**inputs)
            loss = outputs.loss
            
            warmup_optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            warmup_optimizer.step()
            
            total_loss += loss.item()
        
        avg_loss = total_loss / num_steps
        print(f"âœ… SFT Warmup complete! Avg loss: {avg_loss:.4f}")
        return avg_loss
    
    def _get_loss_components(self, prompts, responses, rewards, reference_logprobs):
        """è·å–æŸå¤±ç»„ä»¶ï¼ˆpolicy loss å’Œ KL divergenceï¼‰"""
        try:
            # é‡æ–°è®¡ç®—ä¸€æ¬¡ä»¥è·å–ç»„ä»¶ï¼ˆä¸æ•ˆç‡ä½†ç®€å•ï¼‰
            # æˆ–è€…å¯ä»¥ä¿®æ”¹ astar_po.compute_loss è¿”å›æ›´å¤šå€¼
            import numpy as np
            
            # ç®€å•ä¼°è®¡ï¼špolicy loss â‰ˆ -mean(reward)
            all_rewards = []
            for reward_list in rewards:
                all_rewards.extend(reward_list)
            
            if len(all_rewards) > 0:
                policy_loss_est = -np.mean(all_rewards)
                # KL å¯ä»¥ä» reference logprobs ä¼°è®¡
                kl_div_est = 0.0  # ç®€åŒ–ï¼Œå®é™…éœ€è¦æ›´å¤æ‚çš„è®¡ç®—
                return float(policy_loss_est), float(kl_div_est)
        except:
            pass
        
        return None, None
    
    def _save_checkpoint(self, iteration: int, loss: float):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        import os
        from datetime import datetime
        
        checkpoint = {
            'iteration': iteration,
            'global_step': self.global_step,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'loss': loss,
            'timestamp': datetime.now().isoformat()
        }
        
        os.makedirs('checkpoints', exist_ok=True)
        checkpoint_path = f"checkpoints/online_checkpoint_iter_{iteration}_step_{self.global_step}.pt"
        torch.save(checkpoint, checkpoint_path)
        print(f"ğŸ’¾ Checkpoint saved: {checkpoint_path}")
